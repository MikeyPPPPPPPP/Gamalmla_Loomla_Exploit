#!/usr/local/bin python3
import requests
from urllib.parse import urlparse
import re


class LinkProcessor:
    def __init__(self, pagein):
        self.page_content = pagein
        self.a_tag_pattern = r"<a\s+(?:[^>]*?\s+)?href=([\"'])(.*?)\1"
        self.check_links = []
        self.good_links = []
        self.extract_links()
    
    def extract_links(self):
        a_tags = re.findall(self.a_tag_pattern, self.page_content)
        for tag in a_tags:

            link = tag[1].strip()
            if link.startswith('http'):
                self.good_links.append(link)
            elif link.startswith('/..') or link.startswith('../'):
                self.check_links.append(link)
            elif link.startswith('/'):
                self.good_links.append(link)
            elif "/" not in link and "." in link:
                self.good_links.append(link)
            
    
    def finish_partial_links(self, base_url, links):
        completed_links = []
        for link in links:
            new_link = base_url
            start = False
            for char in range(len(link)):
                if start:
                    new_link += link[char]
                elif link[char] == "/" and link[char + 1] not in ['.', '/']:
                    new_link += link[char]
                    start = True
            completed_links.append(new_link)
        return completed_links
    
    def get_all_links(self, base_url):
        completed_check_links = self.finish_partial_links(base_url, self.check_links)
        return self.good_links + completed_check_links






class SPIDER:
    """attempts to spider a site for a tags
    
    :base_url: str
    :depth: int
    """
    #leak_occurence_rate_log = {}


    def __init__(self, base_url: str, depth: int = 100) -> None:

        self.base_url = self.build_base_url(base_url) # https://stackoverflow.com

        self.depth = depth
        self.all_urls_found = []
        self.leak_occurence_rate_log: dict = {}
        self.vulnerable_page = ""


    def build_base_url(self, url: str) -> str:
        """make the base url"""
        sch = urlparse(url)[0]
        domain = urlparse(url)[1]
        return sch + "://" + domain


    def parser(self, page: str, url: str):
        
        master_cheff: dict = {}
        all_matching_tags: list = []

        for line in page.split("\n"):
            for tag in ["[driver]", "[host]", "[password]", "[database]", "[port]", "[user]"]:
                if tag in line:
                    all_matching_tags.append(line)

        for patern in all_matching_tags[-50:]:
            line = str(patern).strip()
            configKey = line.split("] => ")

            match(configKey[0].lstrip('[')):
                case "host":
                    master_cheff["host"] = configKey[1]
                case "user":
                    master_cheff["user"] = configKey[1]
                case "password":
                    master_cheff["password"] = configKey[1]
                case "database":
                    master_cheff["database"] = configKey[1]
                case "host":
                    master_cheff["host"] = configKey[1]
                case "port":
                    master_cheff["port"] = configKey[1]
                case "driver":
                    master_cheff["driver"] = configKey[1]
                case default:
                    pass

        #SPIDER.leak_occurence_rate_log = master_cheff
        if len(master_cheff) > len(self.leak_occurence_rate_log):
            self.vulnerable_page = url
            self.leak_occurence_rate_log = master_cheff


    def soup_parser(self, url: str) -> str:
        """returns a bs4 object for parsing"""

        get = requests.get(url, headers = {"User-Agent":"Fight, The power -- Fight, the power"}, verify=False)
        #print(get.text)
        self.parser(get.text, url)

        return get.text




    def return_list_of_hrefs(self, page: str) -> list[str]:
        processor = LinkProcessor(page)
        return processor.get_all_links(self.base_url)


    def validate_url(self, url):
        """this will check if there is a http, /, mailto: after the first http://dgfd/ 
        
        there was a problem with it messing up like this 
        https://erowid.org/https://erowid.org/donations/donations_single.php?src=splashsd1

        """
        counter = 0

        count = 0
        for x in url:
            if count == 3:
                continue

            if x == '/':
                count += 1

            counter += 1

        if url[counter:].startswith('/'):
            return False
        if url[counter:].startswith('http'):
            return False
        if url[counter:].startswith('mailto:'):
            return False
            
        return True

    def proccess_links(self, link: str) -> str:
        """Makes urls valid"""

        #dose the url start with  the base path
        final_url = ""

        #if the url starts with the base url
        if link.startswith(self.base_url):# and self.valid_url(link):
            final_url = link
            return final_url

        #if the url starts with a /
        if link.startswith('/') and link.startswith('http') == False and self.validate_url(self.base_url +link):#and self.valid_url(self.base_url + link):
            final_url = self.base_url +link
            return final_url

        #if the url dose not starts with a / or http
        if link.startswith('/') == False and link.startswith('http') == False and self.validate_url(self.base_url + '/' +link):
            final_url = self.base_url + '/' +link
            return final_url

        

    def recursive_spider(self, urls: list[str] = None) -> list[str]:
        """recursivly spider pages untill specified depth is reached"""
        if self.depth == 0:
            #if we get to the bottom
            return 
        
        self.depth -= 1

        #first inital check
        if urls == None:
            #soup = self.soup_parser(self.base_url)
            hrefs = self.return_list_of_hrefs(self.soup_parser(self.base_url))#soup)

            #this list contains strings and None values 
            temp_links = [self.proccess_links(links) for links in hrefs]

            #this list if like temp_links with the Nones removed
            good_links = [links for links in temp_links if links != None]

            #this will remove doubles
            self.all_urls_found = [*set(good_links)]

            self.recursive_spider(self.all_urls_found)

        if urls == None:
            return

        for link in urls:

            #we can possibly add page analyses functions here, like finding inputs and forums
            #soup = self.soup_parser(link)
            hrefs = self.return_list_of_hrefs(self.soup_parser(link))#soup)
            
            #this list contains strings and None values that are not already in self.all_urls_found
            temp_links = [self.proccess_links(links) for links in hrefs if links not in self.all_urls_found]

            #this list if like temp_links with the Nones removed
            good_links = [links for links in temp_links if links != None]

            #this will be a list of links we want to do more reursion on
            more_links = []

            for link in good_links:
                if link not in self.all_urls_found:
                    more_links.append(link)

            t = good_links + self.all_urls_found

            self.all_urls_found = [*set(t)]


            self.recursive_spider(more_links)


#https://meetings.portseattle.org/index.php
#https://www.pmpa.com/#
